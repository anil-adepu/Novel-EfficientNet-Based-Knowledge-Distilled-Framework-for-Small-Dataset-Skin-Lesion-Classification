{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip >> /dev/null\n!pip install -U --pre efficientnet >> /dev/null\n!pip install tensorflow-addons\n\nimport tensorflow as tf\n#import tensorflow as tf\nfrom tensorflow.python.framework.ops import disable_eager_execution\n\n#disable_eager_execution()\n#tf.enable_eager_execution()\nimport efficientnet.tfkeras as efn\nimport tensorflow_addons as tfa\n\nimport pandas as pd\nimport numpy as np\nimport gc # garbage collection\nfrom kaggle_datasets import KaggleDatasets\nimport re, math\nimport tensorflow.keras.backend as K\nfrom tensorflow.python.keras.utils import losses_utils\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt\nimport pickle\nimport random as r\nimport cv2, os\nimport cv2, pandas as pd, matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# DEVICE = \"TPU\" # or \"GPU\"\n# SEED = 42 # USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\n# FOLDS = 4 # NUMBER OF FOLDS. USE 3, 5, OR 15 \n# IMG_SIZES = [384] * FOLDS\n# BATCH_SIZES = [32] * FOLDS\n# EPOCHS = [40] * FOLDS\n# EFF_NETS = [5] * FOLDS\n# WGTS = [1 / FOLDS] * FOLDS # WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\n# TTA = 15 # TEST TIME AUGMENTATION FACTOR\n# NUMOFCLASSES = 8\n\n\n\nDEVICE = \"TPU\" # or \"GPU\"\nSEED = 42 # USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\nFOLDS = 5 # NUMBER OF FOLDS. USE 3, 5, OR 15 \nIMG_SIZES = [384] * FOLDS\nBATCH_SIZES = [32] * FOLDS\nEPOCHS = [60] * FOLDS\nEFF_NETS = [5] * FOLDS\nWGTS = [1 / FOLDS] * FOLDS # WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\nTTA = 15 # TEST TIME AUGMENTATION FACTOR\nNUMOFCLASSES = 7\nCLASSWEIGHTS = {0:1.28545758 , 1: 0.21338021,2:2.78349083 ,3:4.37527304 ,4:1.30183284,5:12.44099379,6:10.07545272} \n\n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')\n\n\n\n\ntrain = pd.read_csv('/kaggle/input/ham10k-withtest-stratified5fold-inpainted384x384/Ham10k_384_Inpainted_5FoldStratified/HAM10000_Train_Edited.csv')\n\n# GCS_PATH  = [None]*FOLDS\n# for i,k in enumerate(IMG_SIZES):\n#     GCS_PATH[i] = KaggleDatasets().get_gcs_path('isic-2019-train-384x384-inpainted-stratified-tfrec')\n\n# files_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/stratified_jpg_384_inpainted_tfrec/train*.tfrec')))\n# files_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/stratified_jpg_384_inpainted_tfrec/train04-5062.tfrec')))\n# #files_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/test*.tfrec')))\n\nGCS_PATH  = [None]*FOLDS\nfor i,k in enumerate(IMG_SIZES):\n    #print(i,k)\n    GCS_PATH[i] = KaggleDatasets().get_gcs_path('ham10k-minimal')\n    #GCS_PATH[i] = KaggleDatasets().get_gcs_path('ham10k-withtest-stratified5fold-inpainted384x384')\n    #GCS_PATH[i] = '/kaggle/input/ham10k-withtest-stratified5fold-inpainted384x384'\n\n    \n    \n#files_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/Tfrecs_with_only_int_and_bytes_encoding/withoutstringencode/train*.tfrec')))\n#files_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/Tfrecs_with_only_int_and_bytes_encoding/withoutstringencode/test*.tfrec')))\n#files_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/test*.tfrec')))\nfiles_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/train*.tfrec')))\nfiles_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/test*.tfrec')))\n\n\nROT_ = 180.0\nSHR_ = 2.0\nHZOOM_ = 8.0\nWZOOM_ = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear    = math.pi * shear    / 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n                               zero,            one/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\n\ndef transform(image, DIM=256):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n    return tf.reshape(d,[DIM, DIM,3])\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T17:39:04.91168Z","iopub.execute_input":"2023-02-19T17:39:04.912386Z","iopub.status.idle":"2023-02-19T17:39:52.662873Z","shell.execute_reply.started":"2023-02-19T17:39:04.912342Z","shell.execute_reply":"2023-02-19T17:39:52.661981Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_labeled_tfrecord(example):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n        'target'                       : tf.io.FixedLenFeature([], tf.string),\n        'target_id'                    : tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    label = tf.cast(example['target_id'], tf.int64)\n    #print(\"Example: \",example,\"Done\")\n    return (example['image'], label)\n\ndef read_unlabeled_tfrecord(example , return_image_name):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n        'target'                       : tf.io.FixedLenFeature([], tf.string),\n        'target_id'                    : tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    #label = tf.cast(example['multitarget_label'], tf.int32)\n    return (example['image'], example['image_name'] if return_image_name else 0)\n\ndef read_test_unlabeled_tfrecord(example, return_image_name):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string)\n       # 'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n       # 'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n       # 'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n       # 'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n       # 'width'                        : tf.io.FixedLenFeature([], tf.int64),\n       # 'height'                       : tf.io.FixedLenFeature([], tf.int64)\n        \n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return (example['image'], example['image_name'] if return_image_name else 0)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef prepare_image(img, augment=False, dim=384):  \n#    print(\"In Prepare Image \",img,\"pri done\")\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.cast(img, tf.float32) / 255.0\n    img = tf.image.resize(img, size=[dim, dim])\n    #cv2.imwrite(\"/kaggle/working/img.jpg\", img_np)\n    #img = tf.image.per_image_standardization(img)    \n    #print(\"In Prepare Image22\")\n    if augment:\n        #print(\"In Prepare Image33\")\n        img = transform(img,DIM=dim)\n        #print(\"In Prepare Image334\")\n        img = tf.image.random_flip_left_right(img)\n        #print(\"In Prepare Image335\")\n        img = tf.image.random_flip_up_down(img)\n        #print(\"In Prepare Image336\")\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        #print(\"In Prepare Image337\")\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        #print(\"In Prepare Image338\")\n        img = tf.image.random_brightness(img, 0.1)\n        #print(\"In Prepare Image339\")\n        #img = sprinkles(img)\n        #img = microscopicCutOut(img)\n    #print(\"In Prepare Image44\")\n    img = tf.reshape(img, [dim, dim, 3])\n    #print(\"returing IMG \",img)\n    return img\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files\n    sum=0\n    for filename in filenames:\n        n = filename.split('-')[-1]\n        n = n.split('.')[0]\n        sum = sum + int(n)\n    return sum\n    #n = [int(re.compile(r\"-([0-9]*).\").search(filename).group(1)) for filename in filenames]\n    #return np.sum(n)\n    \ndef get_dataset(files, augment = False, shuffle = False, repeat = False, \n                labeled=True,return_image_names=True, batch_size=8, dim=384):\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n    \n    if repeat:\n        ds = ds.repeat()\n    \n    if shuffle: \n        ds = ds.shuffle(1024*8)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n        \n    if labeled: \n        #ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n        ds = ds.map(lambda example: read_labeled_tfrecord(example), \n                    num_parallel_calls=AUTO)\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), \n                    num_parallel_calls=AUTO)      \n    #print(\"Going to map againQQQQQQQQQQQQQQQQQQQQQ\")\n    ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, dim=dim), \n                                               imgname_or_label), \n                num_parallel_calls=AUTO)\n    \n    #print(\"DSS outside: \",ds)\n    \n    ds = ds.batch(batch_size * REPLICAS)\n    ds = ds.prefetch(AUTO)\n    return ds\n\n\ndef get_test_dataset(files, augment = False, shuffle = False, repeat = False, \n                labeled=True,return_image_names=True, batch_size=8, dim=384):\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n    \n    if repeat:\n        ds = ds.repeat()\n    \n    if shuffle: \n        ds = ds.shuffle(1024*8)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n        \n    #if labeled: \n    #    ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n        #ds = ds.map(lambda example: read_labeled_tfrecord(example, return_target), \n                    #num_parallel_calls=AUTO)\n    #else:\n    ds = ds.map(lambda example: read_test_unlabeled_tfrecord(example, return_image_names), \n                    num_parallel_calls=AUTO)      \n    \n    ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, dim=dim), \n                                               imgname_or_label), \n                num_parallel_calls=AUTO)\n    \n    #print(\"Printing ds : \" ,ds)\n    \n    ds = ds.batch(batch_size * REPLICAS)\n    ds = ds.prefetch(AUTO)\n    return ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nclass Distiller(tf.keras.Model):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.5,\n        temperature=1,\n    ):\n\n        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n    def train_step(self, data):\n        # Unpack data\n        x, y_ = data\n        y = y_\n        #y = tf.reshape(y_, (-1,7))\n\n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n\n            student_predictions = self.student(x, training=True)\n\n\n            loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n            per_example_loss = loss_object(y, student_predictions)\n            student_loss = tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZES[fold]*REPLICAS)\n            \n            teacher_logit = (tf.math.log(teacher_predictions) - tf.math.log(1-teacher_predictions))\n            teacher_predictions_soft = tf.math.sigmoid(teacher_logit/self.temperature)\n            student_logit = (tf.math.log(student_predictions) - tf.math.log(1-student_predictions))\n            student_predictions_soft = tf.math.sigmoid(student_logit/self.temperature)\n\n#             # L_soft\n#             per_example_loss_ = self.distillation_loss_fn(\n#                 teacher_predictions_soft,\n#                 student_predictions_soft,\n#             )\n\n            # L_soft\n            per_example_loss_ = self.distillation_loss_fn(\n                teacher_predictions,\n                student_predictions,\n            )\n            \n            distillation_loss = tf.nn.compute_average_loss(per_example_loss_, global_batch_size=BATCH_SIZES[fold]*REPLICAS)\n\n\n            loss = (self.alpha) * distillation_loss + (1 - self.alpha) * student_loss\n\n\n\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n\n        self.compiled_metrics.update_state(y, student_predictions)\n\n\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n    \n    def test_step(self, data):\n        # Unpack the data\n        x, y_ = data\n        y = y_\n        #y = tf.reshape(y_, (-1,7))\n        # Compute predictions\n        y_prediction = self.student(x, training=False)\n\n\n        \n        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n        per_example_loss = loss_object(y, y_prediction)\n        student_loss = tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZES[fold]*REPLICAS)\n\n\n        self.compiled_metrics.update_state(y, y_prediction)\n\n\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\n    def call(self, data):\n        y_pred = self.student(data) \n        return y_pred\n\n    @tf.function\n    def distributed_train_step(dist_inputs):\n        per_replica_losses = strategy.run(train_step, args=(dist_inputs,))\n        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n                             axis=None)\n\n    @tf.function\n    def distributed_test_step(dist_inputs):\n        return strategy.run(test_step, args=(dist_inputs,))","metadata":{"execution":{"iopub.status.busy":"2023-02-19T17:39:52.664864Z","iopub.execute_input":"2023-02-19T17:39:52.665892Z","iopub.status.idle":"2023-02-19T17:39:52.966609Z","shell.execute_reply.started":"2023-02-19T17:39:52.665835Z","shell.execute_reply":"2023-02-19T17:39:52.965554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\ndef get_EFF_NET(dim=128, ef=0, output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n    \n    inp = tf.keras.layers.Input(shape=(dim,dim,3), name='inp')\n    base = EFNS[ef](input_shape=(dim,dim,3),weights='noisy-student',include_top=False)\n    base.trainable = True\n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(7,activation='softmax', bias_initializer=output_bias)(x)\n    \n    #print(\"Hey\")\n    #print(x)\n    model = tf.keras.Model(inputs=[inp],outputs=[x])\n    opti = tfa.optimizers.RectifiedAdam(learning_rate=0.00032, total_steps=10000,\n                               warmup_proportion=0.1, min_lr=1e-7)\n    #loss = tfa.losses.SigmoidFocalCrossEntropy(alpha=0.90,gamma=2.0,\n                                    #reduction=losses_utils.ReductionV2.NONE)\n    loss= tf.keras.losses.SparseCategoricalCrossentropy()\n\n    #METRICS = [tf.keras.metrics.SparseCategoricalAccuracy(), tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='SEN/Recall'),'accuracy']\n    METRICS = [tf.keras.metrics.SparseCategoricalAccuracy()]\n    model.compile(optimizer=opti, loss=loss, metrics=METRICS)\n    print(model.summary())\n#     model.summary()\n    \n    return model\n\ndef get_lr_callback(batch_size=8):    \n    lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_sparse_categorical_accuracy', factor=0.3, patience=2, verbose=2,\n        mode='max', min_delta=0.001, min_lr=0.000000001\n    )\n\n    return lr_callback\n\nVERBOSE = 1\nDISPLAY_PLOT = False\n\nskf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n\noof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = []\npreds = np.zeros((count_data_items(files_test),NUMOFCLASSES))\npreds1 = np.zeros((count_data_items(files_test),NUMOFCLASSES))\npred_foldWise = np.asarray([preds]*FOLDS)\n\n#CLASSES = ['NV', 'MEL', 'BKL', 'DF', 'SCC', 'BCC', 'VASC', 'AK']\nCLASSES = ['mel', 'nv', 'bcc', 'akiec', 'bkl', 'df', 'vasc']\n\nVERBOSE = 1\nDISPLAY_PLOT = False\n\nskf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n\noof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = []\npreds = np.zeros((count_data_items(files_test),NUMOFCLASSES))\npreds1 = np.zeros((count_data_items(files_test),NUMOFCLASSES))\npred_foldWise = np.asarray([preds]*FOLDS)\n\nfor fold,(idxT,idxV) in enumerate(skf.split(np.arange(5))):    \n    if DEVICE=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print('#'*25); print('#### FOLD',fold+1)\n    print('#### Image Size %i and batch_size %i'%\n          (IMG_SIZES[fold], BATCH_SIZES[fold]*REPLICAS))\n    \n    # CREATE TRAIN AND VALIDATION SUBSETS\n    files_train = tf.io.gfile.glob([GCS_PATH[fold] + '/train0%.1i*.tfrec'%x for x in idxT])\n\n    np.random.shuffle(files_train); print('#'*25)\n    files_valid = tf.io.gfile.glob([GCS_PATH[fold] + '/train0%.1i*.tfrec'%x for x in idxV])\n    files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + '/test*.tfrec')))\n\n    # SAVE BEST MODEL EACH FOLD\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'Dis_v27E5E0NoTemp_0_4_Alpha_384Dim_TTA15_fold-%i.h5'%fold, monitor='val_sparse_categorical_accuracy', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='max', save_freq='epoch')\n    \n    # early stopping with 5 patience\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_sparse_categorical_accuracy', mode = 'max', patience = 5, \n                          verbose = 2, min_delta = 0.0001, restore_best_weights = True)\n\n    a = count_data_items(files_train); b=count_data_items(files_valid);\n    print('AUGMENTED TRAIN SIZE: ', a, ' * ', TTA, ' = ', a*TTA)\n    print('AUGMENTED VALID SIZE: ', b, ' * ', TTA, ' = ', b*TTA)\n    # BUILD MODEL\n    K.clear_session()\n    with strategy.scope():\n        opt = tfa.optimizers.RectifiedAdam(lr=0.00032, total_steps=10000,\n                               warmup_proportion=0.1, min_lr=1e-7)\n        opti = tfa.optimizers.Lookahead(opt, sync_period=5, slow_step_size=0.8)\n        #loss = tfa.losses.SigmoidFocalCrossEntropy(alpha=0.90,gamma=2.0,\n        #                                reduction=losses_utils.ReductionV2.NONE)\n        loss= tf.keras.losses.SparseCategoricalCrossentropy()\n\n\n        METRICS = [tf.keras.metrics.SparseCategoricalAccuracy()]\n        teacher = get_EFF_NET(dim=IMG_SIZES[fold],ef=5, output_bias = None)\n        student = get_EFF_NET(dim=IMG_SIZES[fold],ef=0, output_bias = None)\n        # Initialize and compile distiller\n        distiller = Distiller(student=student, teacher=teacher)\n        # Clone student for later comparison\n        student_scratch = tf.keras.models.clone_model(student)\n\n        teacher.load_weights('/kaggle/input/hame5output/HAMe5fold-2.h5')\n\n        distiller.compile(\n            optimizer=opti,\n            metrics=METRICS,\n            student_loss_fn=loss,\n            distillation_loss_fn=tf.keras.losses.KLDivergence(\n                        reduction=losses_utils.ReductionV2.NONE),\n            alpha=0.40,\n            temperature=1,\n        )\n    # Distill teacher to student\n    print('Distilling teacher to student....(Training)...')\n    history = distiller.fit(\n        get_dataset(files_train, augment=True, shuffle=True, repeat=True, \n                    dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]),\n        epochs = EPOCHS[fold],\n        #epochs = 3,\n        callbacks = [sv, early_stopping, get_lr_callback(BATCH_SIZES[fold])], \n        steps_per_epoch = (count_data_items(files_train))/BATCH_SIZES[fold]//REPLICAS,\n        validation_data = get_dataset(files_valid,augment=False,shuffle=False, repeat=False,\n                                      dim=IMG_SIZES[fold]),\n        verbose = VERBOSE\n    )\n    #history.history['val_student_loss'] = np.mean(history.history['val_student_loss'], axis=1)\n\n    distiller.built = True\n    \n    print('Loading best student model...')\n    distiller.load_weights('Dis_v27E5E0NoTemp_0_4_Alpha_384Dim_TTA15_fold-%i.h5'%fold)\n    \n    # PREDICT OOF USING TTA\n    print('Predicting OOF with TTA...')\n    ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ct_valid = (count_data_items(files_valid)); STEPS = TTA * ct_valid/BATCH_SIZES[fold]/4/REPLICAS\n    pred = distiller.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,]\n    \n#     oof_pred.append( np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1))\n    oof_pred.append( np.mean(pred.reshape((ct_valid,TTA,NUMOFCLASSES),order='F'),axis=1))\n    \n    # GET OOF TARGETS AND NAMES\n    ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n            labeled=True, return_image_names=True)\n    oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\n    oof_folds.append( np.ones_like(oof_tar[-1],dtype='int8')*fold )\n    ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n                labeled=False, return_image_names=True)\n    oof_names.append( np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())]))\n    \n\n    \n    # PREDICT TEST with TTA\n    print('Predicting Test with TTA...')\n#     ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n#             repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ds_test = get_test_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n\n    ct_test = count_data_items(files_test); STEPS = TTA * ct_test/BATCH_SIZES[fold]/4/REPLICAS\n    pred = distiller.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,]\n\n    \n    tmp_pred = np.mean(pred.reshape((ct_test,TTA,NUMOFCLASSES),order='F'),axis=1)\n    preds += tmp_pred * WGTS[fold]\n    \n    # PREDICT TEST without TTA\n    print('Predicting Test without TTA...')\n#     ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=False,\n#             repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ds_test = get_test_dataset(files_test,labeled=False,return_image_names=False,augment=False,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ct_test = count_data_items(files_test); STEPS = 1 * ct_test/BATCH_SIZES[fold]/4/REPLICAS\n    pred = distiller.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:1*ct_test,]\n#     tmp_pred1 = np.mean(pred.reshape((ct_test,1),order='F'),axis=1)\n#     preds1[:,0] += tmp_pred1 * WGTS[fold]\n\n#     pred_foldWise[fold][:,0] += tmp_pred\n    \n    tmp_pred1 = np.mean(pred.reshape((ct_test,1,NUMOFCLASSES),order='F'),axis=1)\n    preds1 += tmp_pred1 * WGTS[fold]\n\n    pred_foldWise[fold] += tmp_pred\n\n    hist=dict(zip(list(history.history.keys()), np.array(list(history.history.values()))))\n    pickle.dump(hist, open(\"Dis_v27E5E0NoTemp_0_4_Alpha_384Dim_TTA15_history_fold-%i.p\"%(fold+1), \"wb\"))\n    # PLOT TRAINING\n    if DISPLAY_PLOT:\n        plt.figure(figsize=(15,5))\n        epoch_new=len(list(history.history.values())[0])\n        plt.plot(np.arange(epoch_new),history.history['AUC'],'-o',label='Train AUC',color='#ff7f0e')\n        plt.plot(np.arange(epoch_new),history.history['val_AUC'],'-o',label='Val AUC',color='#1f77b4')\n        x = np.argmax( history.history['val_AUC'] ); y = np.max( history.history['val_AUC'] )\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.4f'%y,size=14)\n        plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(np.arange(epoch_new),history.history['student_loss'],'-o',label='Train Loss',color='#2ca02c')\n        plt2.plot(np.arange(epoch_new),history.history['val_student_loss'],'-o',label='Val Loss',color='#1f77b4')\n        x = np.argmin( history.history['val_student_loss'] ); y = np.min( history.history['val_student_loss'] )\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss', size=14)\n        plt.ylabel('Loss',size=14)\n        plt.title('FOLD %i - Image Size %i, EFF_NETB%i'% (fold+1,IMG_SIZES[fold],EFF_NETS[fold]), size=18)\n        plt.legend(loc=3)\n        plt.show()\n\n    del student; del teacher; del distiller; z = gc.collect()\n    #break","metadata":{"execution":{"iopub.status.busy":"2023-02-19T17:39:52.968133Z","iopub.execute_input":"2023-02-19T17:39:52.968507Z","iopub.status.idle":"2023-02-19T17:39:52.979111Z","shell.execute_reply.started":"2023-02-19T17:39:52.968462Z","shell.execute_reply":"2023-02-19T17:39:52.978235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COMPUTE OVERALL OOF AUC\noof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\nnames = np.concatenate(oof_names); folds = np.concatenate(oof_folds)\nauc = roc_auc_score(true,oof, multi_class='ovr')\nprint('Overall OOF AUC with TTA = %.4f'%auc)\n\n# SAVE OOF TO DISK\ndf_oof = pd.DataFrame(dict(\n    image_name = names, target=true, targetA=oof[:,0], targetB=oof[:,1], targetC=oof[:,2], targetD=oof[:,3], targetE=oof[:,4], targetF=oof[:,5], targetG=oof[:,6], fold=folds))\ndf_oof.to_csv('HAMDis_v27E5E0NoTemp_0_4_Alpha_384Dim_TTA15_oof.csv',index=False)\n# df_oof.head()\n\nds = get_test_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold],\n                 labeled=False, return_image_names=True)\n#ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold],\n#                 labeled=False, return_image_names=True)\n\nimage_names = np.array([img_name.numpy().decode(\"utf-8\")\n                        for img, img_name in iter(ds.unbatch())])\n\nsubmission = pd.DataFrame(dict(image_name=image_names, targetA=preds1[:,0], targetB=preds1[:,1], targetC=preds1[:,2], targetD=preds1[:,3], targetE=preds1[:,4], targetF=preds1[:,5], targetG=preds1[:,6]))\nsubmission = submission.sort_values('image_name')\nsubmission.to_csv('HAMDis_v27E5E0NoTemp_0_4_Alpha_384Dim_TTA15_ithout_TTA_submission.csv', index=False)\nprint(submission.head())\n\nsubmission = pd.DataFrame(dict(image_name=image_names, targetA=preds[:,0], targetB=preds[:,1], targetC=preds[:,2], targetD=preds[:,3], targetE=preds[:,4], targetF=preds[:,5], targetG=preds[:,6]))\nsubmission = submission.sort_values('image_name')\nsubmission.to_csv('HAMDis_v27E5E0NoTemp_0_4_Alpha_384Dim_TTA15_with_TTA_submission.csv', index=False)\nprint(submission.head())\n\nnp.save('HAMD_v27E5E0NoTemp_0_4_Alpha_384Dim_TTA15_iswfoldWisePredictions.npy', pred_foldWise)\n\n##This is v4 alpha = 0.55","metadata":{"execution":{"iopub.status.busy":"2023-02-19T17:39:52.981716Z","iopub.execute_input":"2023-02-19T17:39:52.982402Z","iopub.status.idle":"2023-02-19T17:39:58.545153Z","shell.execute_reply.started":"2023-02-19T17:39:52.982353Z","shell.execute_reply":"2023-02-19T17:39:58.542446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-02-19T17:39:58.546708Z","iopub.execute_input":"2023-02-19T17:39:58.546949Z","iopub.status.idle":"2023-02-19T17:39:58.682882Z","shell.execute_reply.started":"2023-02-19T17:39:58.54692Z","shell.execute_reply":"2023-02-19T17:39:58.682009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-02-19T17:39:58.68469Z","iopub.execute_input":"2023-02-19T17:39:58.685066Z","iopub.status.idle":"2023-02-19T17:39:58.69099Z","shell.execute_reply.started":"2023-02-19T17:39:58.685018Z","shell.execute_reply":"2023-02-19T17:39:58.690048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}